{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the basic building blocks?**\n",
    "\n",
    "* **Artificial neurons** - elementary units, combine their inputs to produce an output/activation\n",
    "* **Activation functions** - non-linear transformation applied to the outputs\n",
    "* **Network layers** - organize our network into layers of artificial neurons\n",
    "\n",
    "\"Tinker with a Neural Network in your browser\" - [playground.tensorflow.org][tf-playground]\n",
    "\n",
    "<a href=\"https://playground.tensorflow.org\">\n",
    "    <img src=\"images/tf-playground.png\" width=\"400px\" />\n",
    "</a>\n",
    "\n",
    "[tf-playground]:https://playground.tensorflow.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "import torchvision\n",
    "print(\"Torchvision version:\", torchvision.__version__)\n",
    "\n",
    "import matplotlib\n",
    "print(\"Matplotlib version:\", matplotlib.__version__)\n",
    "\n",
    "import seaborn as sns\n",
    "print(\"Seaborn version:\", sns.__version__)\n",
    "\n",
    "import IPython\n",
    "print(\"IPython version:\", IPython.__version__)\n",
    "\n",
    "import numpy as np\n",
    "print(\"Numpy version:\", np.__version__)\n",
    "\n",
    "import sklearn\n",
    "print(\"Scikit-learn version:\", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Matplotlib and Seaborn\n",
    "%matplotlib inline\n",
    "#%config InlineBackend.figure_format = 'retina' # If you have a retina screen\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set() # Activate Seaborn default style\n",
    "blue, green, red = sns.color_palette()[:3] # Color palette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circle data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# Generate circle data set\n",
    "X, y = make_circles(\n",
    "    n_samples=200, shuffle=True, noise=0.1, random_state=0, factor=0.3)\n",
    "\n",
    "# Rescale data\n",
    "X = scale(X)\n",
    "\n",
    "# Plot data points\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision surface\n",
    "\n",
    "**What is the decision surface?**\n",
    "\n",
    "* What we want to learn in this **classification** task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Plot the decision surface\n",
    "def decision_surface(x1, x2, y, axis, predict_fn, n=100):\n",
    "    # Same scale for x- and y-axis\n",
    "    axis.set_aspect('equal', adjustable='box')\n",
    "\n",
    "    # Plot data points\n",
    "    class1_idx = (y == 1)\n",
    "    styling = {'edgecolors': 'white', 'linewidth': 0.5, 'zorder': 10}\n",
    "    axis.scatter(x1[class1_idx], x2[class1_idx], color=red, label='class 1', **styling)\n",
    "    axis.scatter(x1[~class1_idx], x2[~class1_idx], color=blue, label='class 0', **styling)\n",
    "\n",
    "    if predict_fn is not None:\n",
    "        # Generate grid\n",
    "        xlim, ylim = axis.get_xlim(), axis.get_ylim()\n",
    "        x_values = np.linspace(*xlim, num=n)\n",
    "        y_values = np.linspace(*ylim, num=n)\n",
    "        xx, yy = np.meshgrid(x_values, y_values)\n",
    "        points = np.c_[xx.flatten(), yy.flatten()]\n",
    "        \n",
    "        # Compute predictions\n",
    "        preds = predict_fn(points)\n",
    "        zz = np.array(preds).reshape(xx.shape)\n",
    "\n",
    "        # Draw decision boundary\n",
    "        with warnings.catch_warnings(): \n",
    "            # Matplotlib throws UserWarnings when there are no contour lines to draw\n",
    "            warnings.simplefilter('ignore', category=UserWarning)\n",
    "            axis.contour(xx, yy, zz, levels=[0], colors='gray', zorder=1)\n",
    "\n",
    "        # Plot decision surface\n",
    "        axis.imshow(zz, alpha=0.2, origin='lower', extent=[*xlim, *ylim], vmin=-1, vmax=1, cmap=plt.cm.coolwarm, zorder=1, aspect='auto')\n",
    "        \n",
    "    # Add labels\n",
    "    axis.legend(frameon=True, facecolor='white').set_zorder(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "axis = fig.gca()\n",
    "\n",
    "f = lambda X: X[:, 0]\n",
    "decision_surface(X[:, 0], X[:, 1], y, axis, predict_fn=f)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task:** Improve the decision boundary! ex. Try with the norm `np.linalg.norm(X, ord=2, axis=1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning model\n",
    "\n",
    "**How do neural network learn?**\n",
    "\n",
    "* **Loss function** - A way to quantify how much error the model does\n",
    "* **Gradient descent** - A way to adjust our model to decrease the loss value\n",
    "\n",
    "What does the network learn?\n",
    "\n",
    "* **Parameters** - a set of weights and biases\n",
    "\n",
    "Set of videos about deep learning by Grant Sanderson - in particular [Chapter 3: backpropagation][backprop-video]\n",
    "\n",
    "<a href=\"https://youtu.be/Ilg3gGewQ5U?t=3m46s\">\n",
    "    <img src=\"https://img.youtube.com/vi/Ilg3gGewQ5U/maxresdefault.jpg\" width=\"400px\" />\n",
    "</a>\n",
    "\n",
    "[backprop-video]:https://youtu.be/Ilg3gGewQ5U?t=3m46s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to build and train one in PyTorch?\n",
    "\n",
    "What is PyTorch? Source - [pytorch.org/about][pytorch-about]\n",
    "\n",
    "> PyTorch is a python package that provides two high-level features:\n",
    "> * Tensor computation (like numpy) with strong GPU acceleration\n",
    "> * Deep Neural Networks built on a tape-based autodiff system\n",
    ">\n",
    "> Usually one uses PyTorch either as:\n",
    "> * A replacement for numpy to use the power of GPUs.\n",
    "> * a deep learning research platform that provides maximum flexibility and speed\n",
    "\n",
    "**PyTorch components**\n",
    "\n",
    "<img src=\"images/pytorch-about.png\" width=\"400px\" />\n",
    "\n",
    "[pytorch-about]:https://pytorch.org/about/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a Neural Network\n",
    "\n",
    "**What are the steps to build a network in PyTorch?**\n",
    "\n",
    "```python\n",
    "# Create the model\n",
    "...\n",
    "\n",
    "for epoch in range(10**5):\n",
    "    # Forward pass\n",
    "    ...\n",
    "    \n",
    "    # Backpropagation\n",
    "    ...\n",
    "    \n",
    "    if epoch%100 == 0:\n",
    "        # Plot decision surface\n",
    "        ...\n",
    "```\n",
    "\n",
    "PyTorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=2, out_features=4),\n",
    "    torch.nn.Tanh(),\n",
    "    torch.nn.Linear(in_features=4, out_features=2),\n",
    ")\n",
    "\n",
    "# Criterion and optimizer for \"training\"\n",
    "criterion = torch.nn.CrossEntropyLoss() # Classification\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.03)\n",
    "\n",
    "def forward(X):\n",
    "    # Pass input to the network\n",
    "    X_tensor = torch.FloatTensor(X)\n",
    "    X_variable = torch.autograd.Variable(X_tensor)\n",
    "    output = model(X_variable)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def backpropagation(output):\n",
    "    # Clear the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Compute error\n",
    "    y_tensor = torch.LongTensor(y)\n",
    "    y_variable = torch.autograd.Variable(y_tensor)\n",
    "    loss = criterion(output, y_variable)\n",
    "    \n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # Let the optimizer adjust our model\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.data\n",
    "    \n",
    "# Create a figure to visualize the results\n",
    "# note: you can reduce the figure size it it's too slow\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(8, 3))\n",
    "    \n",
    "try:\n",
    "    loss_values = []\n",
    "    \n",
    "    for epoch in range(10**5):\n",
    "        # Forward pass\n",
    "        output = forward(X)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss = backpropagation(output)\n",
    "        loss_values.append(loss)\n",
    "\n",
    "        if epoch%100 == 0:\n",
    "            # Plot decision surface\n",
    "            ax1.cla()\n",
    "            ax1.set_title('Epoch {}'.format(epoch))\n",
    "            decision_surface(X[:, 0], X[:, 1], y, ax1, lambda X: forward(X)[:, 1].data)\n",
    "            ax2.cla()\n",
    "            ax2.set_title('Loss')\n",
    "            ax2.plot(loss_values)\n",
    "\n",
    "            # Jupyter trick\n",
    "            IPython.display.clear_output(wait=True)\n",
    "            IPython.display.display(fig)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # Clear output\n",
    "    IPython.display.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moons data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Moons data set\n",
    "X, y = make_moons(\n",
    "    n_samples=200, shuffle=True, noise=0.1, random_state=0)\n",
    "\n",
    "# Rescale data\n",
    "X = scale(X)\n",
    "\n",
    "# Plot data points\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "decision_surface(X[:, 0], X[:, 1], y, fig.gca(), predict_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task:** Implement a network with 2 hidden layers, \"ReLU\" activations and train it with a learning rate of 0.1 - [network in playground][moons-network]\n",
    "\n",
    "[moons-network]:http://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.1&regularizationRate=0&noise=0&networkShape=6,4&seed=0.73171&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exclusive or data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"excusive or\" data set\n",
    "X = np.random.uniform(low=-1, high=1, size=(200, 2))\n",
    "y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0).astype(np.int)\n",
    "\n",
    "# Rescale data\n",
    "X = scale(X)\n",
    "\n",
    "# Plot data points\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "decision_surface(X[:, 0], X[:, 1], y, fig.gca(), predict_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small challenge - visualize the output of a hidden unit\n",
    "\n",
    "<!---\n",
    "def f(X, layer_idx, unit_idx):\n",
    "    activation = torch.autograd.Variable(torch.FloatTensor(X))\n",
    "    for layer in model[:layer_idx]:\n",
    "        activation = layer(activation)\n",
    "    return activation[:, unit_idx].data\n",
    "    \n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "decision_surface(X[:, 0], X[:, 1], y, fig.gca(), predict_fn=lambda X: f(X, 1, 0))\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "\n",
    "Nice visualizations\n",
    "\n",
    "* \"Neural network 3D simulation\" - [Video by Denis Dmitriev][3d-simulation]\n",
    "\n",
    "[3d-simulation]:https://www.youtube.com/watch?v=3JQ3hYko51Y\n",
    "\n",
    "To go deeper\n",
    "* Commonly used activation functions - [cs231n course][cs231-actfun]\n",
    "* How the backpropagation algorithm works - [Michael Nielsen's book, Chapter 2][nndl-chap2]\n",
    "* A visual proof that neural nets can compute any function - [Michael Nielsen's book, Chapter 4][nndl-chap4]\n",
    "\n",
    "[cs231-actfun]:http://cs231n.github.io/neural-networks-1/#actfun\n",
    "[nndl-chap2]:http://neuralnetworksanddeeplearning.com/chap2.html\n",
    "[nndl-chap4]:http://neuralnetworksanddeeplearning.com/chap4.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
